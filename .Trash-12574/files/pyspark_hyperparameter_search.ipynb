{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0081004b-7560-459a-9905-b32a89074e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcfe04-ed5b-4c75-9494-4d3239149086",
   "metadata": {},
   "source": [
    "## Spark and Mlflow\n",
    "\n",
    "This is an example of using Spark with Mlflow. It currently *(upto 5.7) only works for on-demand Spark*.\n",
    "\n",
    "**Starting 5.8, it will work for external spark as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251b9589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488496 sha256=85da07e8a39ab48b267f0b70541bb048e184b5a3de18de354f7df6910ad419be\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/92/09/11/aa01d01a7f005fda8a66ad71d2be7f8aa341bddafb27eee3c7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d51bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abe23fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.entities import Param,Metric,RunTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "736b7cd5-4cc8-4669-b574-9f8715ef2ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/07 16:46:45 INFO mlflow.tracking.fluent: Experiment with name 'SPARK-integration-test-Experiment-Manager-Demos' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_name: SPARK-integration-test-Experiment-Manager-Demos\n",
      "experiment_id: 23\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'SPARK'+'-' + os.environ['DOMINO_STARTING_USERNAME'] + '-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "model_name = 'SPARK'+'-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "print(\"experiment_name:\", experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(\"experiment_id:\", experiment_id)\n",
    "\n",
    "import time\n",
    "now = round(time.time())\n",
    "time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n",
    "tags={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cc447",
   "metadata": {},
   "source": [
    "# HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7ba355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(alpha, run_origin, tags={}):\n",
    "    with mlflow.start_run(run_name=run_origin,tags=tags) as run:\n",
    "        print(\"runId:\", run.info.run_uuid)\n",
    "        print(\"artifact_uri:\", mlflow.get_artifact_uri())\n",
    "        print(\"alpha:\", alpha)\n",
    "        print(\"run_origin:\", run_origin)\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_metric(\"rmse\", 0.789)\n",
    "        mlflow.set_tag(\"run_origin\", run_origin)\n",
    "        with open(\"info.txt\", \"w\") as f:\n",
    "            f.write(\"Hi artifact\")\n",
    "        mlflow.log_artifact(\"info.txt\")\n",
    "        params = [ Param(\"p1\",\"0.1\"), Param(\"p2\",\"0.2\") ]\n",
    "        metrics = [ Metric(\"m1\",0.1,now,0), Metric(\"m2\",0.2,now,0) ]\n",
    "        tags = [ RunTag(\"t1\",\"hi1\"), RunTag(\"t2\",\"hi2\") ]\n",
    "        client.log_batch(run.info.run_uuid, metrics, params, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad598c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runId: 3bb7883b53414995a48e273fa48450c1\n",
      "artifact_uri: mlflow-artifacts:/mlflow/3bb7883b53414995a48e273fa48450c1/artifacts\n",
      "alpha: 0.1\n",
      "run_origin: JupyterLab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run(\"0.1\", \"JupyterLab\", tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010bccc",
   "metadata": {},
   "source": [
    "# MLflow for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce59a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b81ed4d1-0346-4acb-b381-3f620705d97a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mlflow#mlflow-spark;1.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mlflow/mlflow-spark/1.11.0/mlflow-spark-1.11.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mlflow#mlflow-spark;1.11.0!mlflow-spark.jar (103ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.25!slf4j-api.jar (24ms)\n",
      ":: resolution report :: resolve 1982ms :: artifacts dl 130ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;1.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b81ed4d1-0346-4acb-b381-3f620705d97a\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (87kB/65ms)\n",
      "24/08/07 16:46:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024/08/07 16:47:12 INFO mlflow._spark_autologging: Autologging successfully enabled for spark.\n"
     ]
    }
   ],
   "source": [
    "import mlflow.spark\n",
    "import os\n",
    "import shutil\n",
    "#from pyspark.sql import SparkSession\n",
    "# Create and persist some dummy data\n",
    "# Note: On environments like Databricks with pre-created SparkSessions,\n",
    "# ensure the org.mlflow:mlflow-spark:1.11.0 is attached as a library to\n",
    "# your cluster\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "df = spark.createDataFrame([\n",
    "        (4, \"spark i j k\"),\n",
    "        (5, \"l m n\"),\n",
    "        (6, \"spark hadoop spark\"),\n",
    "        (7, \"apache hadoop\")], [\"id\", \"text\"])\n",
    "import tempfile\n",
    "tempdir = tempfile.mkdtemp()\n",
    "df.write.csv(os.path.join(tempdir, \"my-data-path\"), header=True)\n",
    "\n",
    "# Enable Spark datasource autologging.\n",
    "mlflow.spark.autolog()\n",
    "loaded_df = spark.read.csv(os.path.join(tempdir, \"my-data-path\"),\n",
    "                header=True, inferSchema=True)\n",
    "# Call toPandas() to trigger a read of the Spark datasource. Datasource info\n",
    "# (path and format) is logged to the current active run, or the\n",
    "# next-created MLflow run if no run is currently active\n",
    "with mlflow.start_run(run_name='spark.autolog sample', tags=tags) as active_run:\n",
    "    pandas_df = loaded_df.toPandas()\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc03c1",
   "metadata": {},
   "source": [
    "# PySpark Hyperparameter Tuning - Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a10ad89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/07 16:47:21 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pyspark.ml. If you encounter errors during autologging, try upgrading / downgrading pyspark.ml to a supported version, or try upgrading MLflow.\n",
      "24/08/07 16:47:32 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2024/08/07 16:48:10 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('words', ArrayType(StringType(), True), True), StructField('features', VectorUDT(), True), StructField('rawPrediction', VectorUDT(), True), StructField('probability', VectorUDT(), True)]. Output schema is not be logged.\n",
      "2024/08/07 16:48:47 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/conda/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "24/08/07 16:48:55 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2665, 0.7335]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9204, 0.0796]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4438, 0.5562]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.8587, 0.1413]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=True, disable=False, exclusive=False, disable_for_unsupported_versions=False, silent=False, log_post_training_metrics=True)\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "mlflow.start_run(run_name='cross-validation', tags=tags)\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)\n",
    "\n",
    "mlflow.end_run()    \n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b06071",
   "metadata": {},
   "source": [
    "# PySpark Hyperparameter Tuning - Train-Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632d6bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/07 16:49:20 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pyspark.ml. If you encounter errors during autologging, try upgrading / downgrading pyspark.ml to a supported version, or try upgrading MLflow.\n",
      "24/08/07 16:49:20 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "24/08/07 16:49:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "2024/08/07 16:49:31 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('features', VectorUDT(), True)]. Model signature is not logged.\n",
      "24/08/07 16:49:57 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...| -17.026492264209548|  -1.780062242348691|\n",
      "|(10,[0,1,2,3,4,5,...|  -16.71909683360509| -0.1893325701092588|\n",
      "|(10,[0,1,2,3,4,5,...| -15.375857723312297|  0.7252323736487188|\n",
      "|(10,[0,1,2,3,4,5,...| -13.772441561702871|  3.2696413241677718|\n",
      "|(10,[0,1,2,3,4,5,...| -13.039928064104615| 0.18817684046065775|\n",
      "|(10,[0,1,2,3,4,5,...|   -9.42898793151394|  -3.449987079269568|\n",
      "|(10,[0,1,2,3,4,5,...|    -9.2679651250406|-0.33109075490696316|\n",
      "|(10,[0,1,2,3,4,5,...|  -9.173693798406978|-0.42727135281551937|\n",
      "|(10,[0,1,2,3,4,5,...| -7.1500991588127265|   2.936884251408867|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.930603551528371|-0.02839768193150...|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.456944198081549| -0.9224776887934015|\n",
      "|(10,[0,1,2,3,4,5,...| -3.2843694575334834| -1.0821208483033875|\n",
      "|(10,[0,1,2,3,4,5,...|   -1.99891354174786|  0.8052068273813595|\n",
      "|(10,[0,1,2,3,4,5,...| -0.4683784136986876|  0.5046267770459569|\n",
      "|(10,[0,1,2,3,4,5,...|-0.44652227528840105|0.053072145020589406|\n",
      "|(10,[0,1,2,3,4,5,...| 0.10157453780074743| -1.0931313634366817|\n",
      "|(10,[0,1,2,3,4,5,...|  0.2105613019270259|   1.068404443672188|\n",
      "|(10,[0,1,2,3,4,5,...|  2.1214592666251364| 0.10226191630980659|\n",
      "|(10,[0,1,2,3,4,5,...|  2.8497179990245116|  1.1908709522287462|\n",
      "|(10,[0,1,2,3,4,5,...|   3.980473021620311|  2.3611397922073025|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=True, disable=False, exclusive=False, disable_for_unsupported_versions=False, silent=False, log_post_training_metrics=True)\n",
    "\n",
    "mlflow.start_run(run_name='train-validation split', tags=tags)\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/mnt/code/data/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()\n",
    "\n",
    "mlflow.end_run()  \n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214afdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Call it to end an active run if there are exceptions\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc1b5b-250f-4cec-8d8f-2fe32633696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc40a-0c8c-426c-a7cb-21525d7e3fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
